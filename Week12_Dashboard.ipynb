{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxeq_J8w5JDw"
      },
      "source": [
        "# NLP Pionner — Twitter Hate Speech Detection\n",
        "\n",
        "## Modeling Families & Business-Aligned Explainable Models\n",
        "\n",
        "**Goal:** Explore one model per family (Linear, Ensemble, Boosting) with business constraint — transparency & interpretability.\n",
        "\n",
        "**Families:**\n",
        "- Linear: Logistic Regression (L1 penalty) — base model, interpretable.\n",
        "- Ensemble: Bagging (Decision Tree) — moderate transparency via permutation importance.\n",
        "- Boosting: Explainable Boosting Machine (EBM) — glass-box additive model.\n",
        "- *(Optional)* Stacking if time allows.\n"
      ],
      "id": "Fxeq_J8w5JDw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2NgPLSP5JDz"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle emoji regex scikit-learn interpret matplotlib\n",
        "from google.colab import files\n",
        "print('Upload kaggle.json (Kaggle > Account > Create API Token)')\n",
        "files.upload()\n",
        "import os, pandas as pd, numpy as np, re, regex, emoji\n",
        "!mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d vkrahul/twitter-hate-speech -p ./data --unzip\n",
        "df = pd.read_csv('./data/train_E6oV3lV.csv')\n",
        "df.columns = [c.lower() for c in df.columns]\n",
        "if 'tweet' in df.columns: df = df.rename(columns={'tweet':'text'})\n",
        "df['text'] = df['text'].fillna('')\n",
        "df.head()"
      ],
      "id": "e2NgPLSP5JDz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU13_V565JDz"
      },
      "outputs": [],
      "source": [
        "# Cleaning\n",
        "URL_RE=re.compile(r'https?://\\S+|www\\.\\S+'); MENTION=re.compile(r'@\\w+'); HASHTAG=re.compile(r'#\\w+'); NUM=re.compile(r'\\d+'); SPACE=re.compile(r'\\s+')\n",
        "def strip_emoji(s): import emoji as em; return em.replace_emoji(s, replace='')\n",
        "def normalize_hashtags(s): return HASHTAG.sub(lambda m: m.group(0)[1:], s)\n",
        "def clean_text(s:str)->str:\n",
        "    s=s.lower(); s=URL_RE.sub(' ',s); s=MENTION.sub(' @user ',s); s=normalize_hashtags(s)\n",
        "    s=NUM.sub(' <num> ',s); s=strip_emoji(s); s=regex.sub(r'[^\\p{L}\\p{N}\\s\\.,!\\?\\']+',' ',s); s=SPACE.sub(' ',s).strip(); return s\n",
        "df['text_clean']=df['text'].astype(str).map(clean_text)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "X_train, X_test, y_train, y_test=train_test_split(df['text_clean'],df['label'],test_size=0.2,stratify=df['label'],random_state=42)\n",
        "tfidf_word=TfidfVectorizer(ngram_range=(1,2),max_features=30000,min_df=2)\n",
        "Xtr=tfidf_word.fit_transform(X_train); Xte=tfidf_word.transform(X_test)"
      ],
      "id": "CU13_V565JDz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7noLXEW5JD0"
      },
      "outputs": [],
      "source": [
        "# 1️⃣ Linear Model: Logistic Regression (L1)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "logreg=LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced',max_iter=400)\n",
        "logreg.fit(Xtr,y_train)\n",
        "pred_lr=logreg.predict(Xte)\n",
        "print('Logistic Regression (L1) Results:\\n')\n",
        "print(classification_report(y_test,pred_lr,digits=3))\n",
        "\n",
        "import numpy as np\n",
        "feature_names=np.array(tfidf_word.get_feature_names_out())\n",
        "coefs=logreg.coef_[0]\n",
        "top_pos_idx=np.argsort(coefs)[-20:][::-1]; top_neg_idx=np.argsort(coefs)[:20]\n",
        "print('\\nTop Positive Features (Hate):')\n",
        "for f,c in zip(feature_names[top_pos_idx],coefs[top_pos_idx]): print(f'{f:25s} {c: .3f}')\n",
        "print('\\nTop Negative Features (Non-Hate):')\n",
        "for f,c in zip(feature_names[top_neg_idx],coefs[top_neg_idx]): print(f'{f:25s} {c: .3f}')"
      ],
      "id": "T7noLXEW5JD0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9U2_INw5JD0"
      },
      "outputs": [],
      "source": [
        "# 2️⃣ Ensemble Model: Bagging (Decision Trees)\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "bag=BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=6,min_samples_leaf=5,class_weight='balanced'),n_estimators=50,random_state=42,n_jobs=-1)\n",
        "bag.fit(Xtr,y_train)\n",
        "pred_bag=bag.predict(Xte)\n",
        "print('Bagging Trees Results:\\n')\n",
        "print(classification_report(y_test,pred_bag,digits=3))\n",
        "\n",
        "# Permutation importance on subset\n",
        "from sklearn.inspection import permutation_importance\n",
        "idx_small=np.random.RandomState(42).choice(Xte.shape[1],size=800,replace=False)\n",
        "Xte_small=Xte[:,idx_small]\n",
        "feat_small=np.array(tfidf_word.get_feature_names_out())[idx_small]\n",
        "perm=permutation_importance(bag,Xte_small.toarray(),y_test,n_repeats=5,random_state=42,n_jobs=-1)\n",
        "order=np.argsort(perm.importances_mean)[::-1][:20]\n",
        "print('\\nTop 20 features by permutation importance (Bagging):')\n",
        "for f,imp in zip(feat_small[order],perm.importances_mean[order]): print(f'{f:25s} {imp: .5f}')"
      ],
      "id": "J9U2_INw5JD0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX9o-Fik5JD0"
      },
      "outputs": [],
      "source": [
        "# 3️⃣ Boosting Model: Explainable Boosting Machine (EBM)\n",
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "ebm=ExplainableBoostingClassifier(interactions=5,outer_bags=8,inner_bags=0,max_bins=256,learning_rate=0.02,validation_size=0.15,random_state=42)\n",
        "Xtr_d=Xtr.tocsc()[:,:20000].astype('float32').toarray()\n",
        "Xte_d=Xte.tocsc()[:,:20000].astype('float32').toarray()\n",
        "ebm.fit(Xtr_d,y_train)\n",
        "pred_ebm=ebm.predict(Xte_d)\n",
        "print('EBM Results:\\n')\n",
        "print(classification_report(y_test,pred_ebm,digits=3))\n",
        "ebm_global=ebm.explain_global()\n",
        "print('Top features (EBM):', ebm_global.data()['names'][:15])"
      ],
      "id": "xX9o-Fik5JD0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM5ShjqF5JD0"
      },
      "outputs": [],
      "source": [
        "# 4️⃣ Optional: Stacking (only if allowed by business)\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "svm=CalibratedClassifierCV(LinearSVC(class_weight='balanced'),cv=5)\n",
        "stack=StackingClassifier(estimators=[('lr',logreg),('bag',bag),('svm',svm)],final_estimator=LogisticRegression(max_iter=300,class_weight='balanced'),n_jobs=-1)\n",
        "stack.fit(Xtr,y_train)\n",
        "pred_stack=stack.predict(Xte)\n",
        "print('Stacking Results:\\n')\n",
        "print(classification_report(y_test,pred_stack,digits=3))"
      ],
      "id": "xM5ShjqF5JD0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRZnxr6e5JD1"
      },
      "source": [
        "## ✅ Model Selection Logic (Business-Aligned)\n",
        "- Prioritize macro-F1 and recall on the hate class.\n",
        "- If EBM performs within 1% of Logistic Regression F1 → **choose EBM** (better nonlinearity + explainability).\n",
        "- Otherwise choose **Logistic Regression (L1)** for full transparency.\n",
        "- Bagging = challenger; use permutation importance and PDPs for interpretability.\n"
      ],
      "id": "ZRZnxr6e5JD1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}